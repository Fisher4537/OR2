\chapter{Meta-heuristics}

\section{Tabu-Search}
Tabu Search (TS) has been developed by Fred Glover \cite{Glover1998}. TS is a heuristic that combines a local search process with a number of anti-cycling rules which prevent the search from getting trapped in a local minimum. Tabu Search is very similar to a local search algorithm that proceeds iteratively from one solution to another through the \texttt{best\_two\_opt} \ref{sec:best_2_opt} method. It also uses a special memory structure, called a tabu list, that determines which subsequent solution will be chosen and therefore to organize the way space is explored. The algorithm starts with a random solution. By applying the \texttt{best\_two\_opt}, as long as a new improvement solution is found, it updates the current solution to it. When it comes to a local minimum (i.e. there are no improvement solutions around two optimality) the use of the tabu list comes into play.\\
Pejorative moves are now allowed and the algorithm chooses the lowest cost one and adds this move to the tabu list, updates the current solution and starts again with the \texttt{best\_two\_opt}.
During the next iteration, while searching for the new solution, check that the move to obtain it does not belong to the list. If it does not appear, then the move is lawful and continues with the next steps of the algorithm. Conversely, if the move is tabu (i.e. not allowed, prohibited), the solution is discarded and another is chosen, until a lawful move is found. The algorithm will continue to choose licit pejorative moves until an improvement one, that does not belong to the list, is found. In this case the algorithm starts again.\\
As will be discussed later, the tabu list has a maximum size of elements. Consequently, when a new element has to be inserted and the list is full, the "oldest" element (i.e. the one that has been inserted longer) of the list will be replaced with it.\\
This strategy allows research to move along the "hills" that the costs of the various solutions form in the solution space. It is an excellent method to escape from the local minimum by maintaining a search based on the \texttt{best\_two\_opt} (Figure \ref{fig:tabu_search_perform_time}).\\
Looking at the implementation details there are many aspects that must be carefully evaluated:

\begin{itemize}
\item Type of structure for tabu list
\item Size of tabu list
\item Termination criterion
\end{itemize}

First of all, the type of structure used for the tabu list. We have decided to propose two different solutions, as each of them has its potential and its defects. The first, more classic, involves the use of a fixed-size circular array to store prohibited moves (so for each \texttt{best\_two\_opt} move, the two relative edges are inserted into the array). The second, however, uses a linked list in which each element is formed by a variable for the edge and a pointer \textit{next} to the next element.
One of the main differences is the allocation of memory, as the linked list dynamically manages its size and therefore aims to optimize space but, on the other hand, the removal of the oldest inserted element requires complete scrolling of the list. Instead, in the circular array the insertion and the deletion of an element is $O(1)$, but on the other hand it has a fixed tabu list dimension.
Fig.s \ref{fig:tabu_search_diff_perform_time} and \ref{fig:tabu_search_diff_perform_lb} show the performance profiles related to the tabu search with the two different data structures (tabu\_search names is related to linked list version). As anticipated, the version with array proves to be the fastest in terms of time, maintaining almost the same quality as the lower bound. \\
\begin{figure}[!h]
	\centering
	\begin{subfigure}{0.49\columnwidth}
		\centering
		\includegraphics[width=\columnwidth]{../res/Ltabu_LA_time.png}
		\caption{Performance profile considering the execution time}
		\label{fig:tabu_search_diff_perform_time}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\columnwidth}
		\centering
		\includegraphics[width=\columnwidth]{../res/Ltabu_LA_lb.png}
		\caption{Performance profile considering the solution cost}
		\label{fig:tabu_search_diff_perform_lb}
	\end{subfigure}
\caption{\texttt{tabu\_search} use linked list as data structure to store the tabu list instead of array.}
\label{fig:tabu_search_diff_perform}
\end{figure}
Another aspect to consider is the size of the tabu list. An "infinite" dimension would risk bringing the algorithm to a standstill in which it can no longer make any move since all those around two optimality have been prohibited. Conversely, too small a size could cause a return to the local minimum from which one was running away. A lot of studies and research have been done for this specific aspect \cite{Nababan_2019, Tsubakitani1998} and a better overall result than others has not been found, changing the typology of the problem also varies the best size of the list. An average list size with respect to the problem proved to be the best choice as a balance between computational time and the optimal cost. Thus a size equal to half the number of problem nodes was set.\\
Finally, the termination criterion can be based on a maximum number of iterations of the algorithm in which the solution no longer improves or on a timelimit.\\
Fig. \ref{fig:tabu_search_perform_time} shows the trend of the lower bounds found over time. Note that the descent and ascent trend is clear in the figure although, after the initial descent, it no longer finds a better solution.
\begin{figure}[h]
	\centering
	\includegraphics[width=.70\columnwidth]{../res/lb_tabu_search_n_greedy_d2103.png}
	\caption{Performance profile considering the execution time.}
	\label{fig:tabu_search_perform_time}
\end{figure}

\section{Simulated-Annealing}
The goal is to find a point in the space at which a real valued energy function (or cost function) is minimized. Simulated annealing is a minimization technique which has given good results in avoiding local minima. This research is based on a \texttt{random\_two\_opt} (similar to sec. \ref{sec:best_2_opt} but choose a random one instead of looking for the best one). Moreover, it follows the idea of taking a random walk through the space at successively lower temperatures, where the probability of taking a step is given by a Boltzmann distribution:
\begin{equation}
	P(\Delta E)= \begin{cases} 
	e^{-\frac{ E_{i+1} - E_{i}}{kT}} & \text{if } E_{i+1} > E_{i} \\
	1 & \text{otherwise}
	\end{cases}
\end{equation}
where $ E_{i} $ is the energy at time $ i $, $ T $ is the temperature, $k$ is the Boltzmann constant and $\Delta E =  E_{i+1} - E_{i}$. \\
\begin{figure}[!h]
	\begin{subfigure}{.5\columnwidth}
		\centering
		\includegraphics[width=\columnwidth]{../res/lb_simulating_annealing_100_a280_500.png}
		\caption{Initial temperature of $100$\%.}
		\label{fig:sim_ann_100}
	\end{subfigure}
	\begin{subfigure}{.5\columnwidth}
		\centering
		\includegraphics[width=\columnwidth]{../res/lb_simulating_annealing_50_a280_500.png}
		\caption{Initial temperature of $50$\%.}
		\label{fig:sim_ann_50}
	\end{subfigure}
	\begin{subfigure}{.5\columnwidth}
		\centering
		\includegraphics[width=\columnwidth]{../res/lb_simulating_annealing_10_a280_500.png}
		\caption{Initial temperature of $10$\%.}
		\label{fig:sim_ann_10}
	\end{subfigure}
	\begin{subfigure}{.5\columnwidth}
		\centering
		\includegraphics[width=\columnwidth]{../res/lb_simulating_annealing_01_a280_500.png}
		\caption{Initial temperature of $1$\%.}
		\label{fig:sim_ann_1}
	\end{subfigure}
	\caption{Performance profiles considering the execution time, with time limit of $500$ seconds and different initial temperatures, of the instance a280.tsp.}
	\label{fig:sim_ann_temperature}
\end{figure}
A step will occur if the new energy is lower. If the new energy is higher, the transition can still occur, and its likelihood is proportional to the temperature $T$ and inversely proportional to the energy difference $\Delta E$.
The temperature $T$ is initially set to a high value (\texttt{INT\_MAX} of \textit{limits.h} C class), and a random walk is carried out at that temperature. Then, the temperature is lowered very slightly according to a cooling schedule.
The cooling schedule chosen is based on the timelimit given to the algorithm, at each interaction the temperature is scaled as a percentage based on the percentage of time spent on the timelimit to perform that iteration. This avoids estimating a fixed value with which to decrease the temperature at each iteration, as the time required for an iteration can vary greatly based on the probability value found (when successful, the computational time increases drastically compared to when probability fails).\\
Furthermore, if the temperature decreases sufficiently slowly, the probability of finding the global minimum tends to 1, thanks to all simulated annealing proofs of convergence in the literature which is based on homogeneous Markov chain theory and the condition of reversibility \cite{Henderson}. Since only improvement steps are accepted at zero temperature, various \texttt{random\_two\_opt} are performed before concluding the algorithm in order to optimize the achieved solution as much as possible.\\
The slight probability of taking a step that gives higher energy is what allows simulated annealing to frequently get out of local minima.

Several single tests were performed for this algorithm focusing on the type of dataset used and the initial temperature. The tests in fig. \ref{fig:sim_ann_temperature} and \ref{fig:sim_ann_dataset} start from an initial tour given by the execution of the Greedy algorithm (\ref{sec:greedy}), while for those in fig. \ref{fig:sim_ann_heavy} an algorithm \texttt{n\_grasp} \ref{sec:grasp} (with n = 2) was used. 
First of all, in fig. \ref{fig:sim_ann_temperature} the different performance profiles are highlighted, based on lower bound and execution time, as the initial temperature varies. It is noted that, starting from a good initial solution, the higher the temperature, the more the simulating annealing will tend to worsen it, accepting pejorative moves with a high probability. However, as the figure shows, this does not bring any advantage compared to starting with a temperature of already 10\%. The theory shows that a very slow descent of the temperature will lead the algorithm to avoid local minimums with probability almost $1$, but already with $500$ seconds it cannot reach the global minimum, indeed, the one with an initial temperature of 10\% succeeds find a lower value.
The case \ref{fig:sim_ann_1} works better than all the others with that dataset as it accepts few pejorative moves (initial ascent of the graph) and spends more time looking for the improvement ones, in fact it is the only one that improves the initial solution given from Greedy.\\
If the initial solution were already in the valley towards a local minimum, the algorithm \ref{fig:sim_ann_1} would hardly be able to get out of it, but instead, if it were in a valley towards the global one then it would almost certainly find the best lower bound. Conversely, it can be said for the \ref{fig:sim_ann_100} algorithm that in the first case it would easily get out of it but most likely it would leave the valley also in the second case.\\
Surely the right initial temperature value is based on the problem.
\begin{figure}[!h]
	\begin{subfigure}{.5\columnwidth}
		\centering
		\includegraphics[width=\columnwidth]{../res/lb_simulating_annealing_100_att48.png}
		\caption{Instance att48.tsp.}
		\label{fig:sim_ann_att48}
	\end{subfigure}
	\begin{subfigure}{.5\columnwidth}
		\centering
		\includegraphics[width=\columnwidth]{../res/lb_simulating_annealing_100_a280.png}
		\caption{Instance a280.tsp.}
		\label{fig:sim_ann_a280}
	\end{subfigure}
	\caption{Two examples of performance profiles considering the execution time with initial temperature of 100\% of different instances.}
	\label{fig:sim_ann_dataset}
\end{figure}

\begin{figure}[!h]
	\begin{subfigure}{.5\columnwidth}
		\centering
		\includegraphics[width=\columnwidth]{../res/lb_simulating_annealing_n_grasp_01_d18512_25200.png}
		\caption{Initial temperature of $1$\% and time limit \protect\\ of $25200$ sec.}
		\label{fig:sim_ann_1_d18512}
	\end{subfigure}
	\begin{subfigure}{.5\columnwidth}
		\centering
		\includegraphics[width=\columnwidth]{../res/lb_simulating_annealing_ngrasp_10_d18512_36000.png}
		\caption{Initial temperature of 10\% and time limit \protect\\ of $36000$ sec.}
		\label{fig:sim_ann_10_d18512}
	\end{subfigure}
	\caption{Performance profile considering the execution time of heavy instance d18512.tsp with different initial temperatures and execution times.}
	\label{fig:sim_ann_heavy}
\end{figure}
There are two other important considerations shown in fig.s \ref{fig:sim_ann_dataset} and \ref{fig:sim_ann_heavy}. The first concerns the search method used, it is clearly seen that \texttt{random\_two\_opt} brings a lot of variability in datasets with few nodes (\ref{fig:sim_ann_att48}), as it is also performed more quickly and therefore more times, while for heavy datasets (\ref{fig:sim_ann_heavy}) the time required for a single \texttt{random\_two\_opt} is very high and the variability is very low. \\
The second consideration concerns only fig. \ref{fig:sim_ann_heavy}, where simulating annealing was tested with the largest dataset of this project. Despite having been performed for many hours and with very low temperatures, it almost never managed to improve the solution, worsening it over time until it ended at the highest lower bound among those found. Due to the high cost of time to execute the \texttt{random\_two\_opt} the algorithm did only a few steps and probably with larger jumps it would have been better, but surely it would have taken a lot of hours to find a good solution to this dataset.


\section{Variable Neighborhood Search (VNS)}

This metaheuristic moves from a solution to another iterating two phases:
\begin{enumerate}
	\item optimization
	\item perturbation
\end{enumerate}

\begin{figure}[!h]
	\begin{subfigure}{.5\columnwidth}
		\centering
		\includegraphics[width=\columnwidth]{../res/lb_vns_n_greedy_10_5_d2103.png}
		\caption{n=5}
		\label{fig:lb_vns_n_greedy_10_95_03_02_5_d2103}
	\end{subfigure}
	\begin{subfigure}{.5\columnwidth}
		\centering
		\includegraphics[width=\columnwidth]{../res/lb_vns_n_greedy_10_10_d2103.png}
		\caption{n=10}
		\label{fig:lb_vns_n_greedy_10_95_03_02_10_d2103}
	\end{subfigure}
	\begin{subfigure}{.5\columnwidth}
		\centering
		\includegraphics[width=\columnwidth]{../res/lb_vns_n_greedy_10_20_d2103.png}
		\caption{n=20}
		\label{fig:lb_vns_n_greedy_10_95_03_02_20_d2103}
	\end{subfigure}
	\begin{subfigure}{.5\columnwidth}
		\centering
		\includegraphics[width=\columnwidth]{../res/d2103_16.png}
		\caption{The graph of the solution returned by VNS (n=5).}
		\label{fig:d2103_16}
	\end{subfigure}
	\caption{The profile of the cost of the solution (vs time domain) returned by \texttt{best\_two\_opt} in each step of the VNS changing the value of n of \texttt{random\_n\_opt}.}
	\label{fig:lb_vns}
\end{figure}
The first is done iterating a refining heuristic until a local minimum is found, in this specific case it has been used the \texttt{best\_two\_opt} algorithm, which is explained in cap \ref{sec:best_2_opt}. After the optimization, a perturbation is applied to leave the local minima, here \texttt{random\_n\_opt}. This algorithm generate a new solution that differ n edges from the first (check at the end of this section for details).
The two phases are iterated until the max number of iterations or time limit occur.\\
Note that the implemented VNS is not optimized to work with multiple thread.\\
In fig \ref{fig:lb_vns} is shown the cost of the solution in each \texttt{2\_opt} step of the VNS using different $ n $ for \texttt{random\_n\_opt}. It is interesting to note that most of the time is required for the first descent and after that the first local minimum is found, the best lower bound does not decrease so much. The calculated tour is plotted in fig \ref{fig:d2103_16}, note that the improvement introduced by VNS w.r.t \texttt{n\_greedy\_best\_two\_opt} is very small.\\
The performance profiles in fig \ref{fig:Lsubtour_hardfixing_vns_time} and \ref{fig:Lsubtour_hardfixing_vns_lb} show that there is no significant difference in using \texttt{n\_greedy}, \texttt{n\_grasp} or a single grasp (\texttt{vsn}) as warm start.

\begin{figure}[!h]
	\begin{subfigure}{.5\columnwidth}
		\centering
		\includegraphics[width=\columnwidth]{../res/Lvns_LA_time.png}
		\caption{Performance profile considering the execution time}
		\label{fig:Lsubtour_hardfixing_vns_time}
	\end{subfigure}
\hfill
	\begin{subfigure}{.5\columnwidth}
		\centering
		\includegraphics[width=\columnwidth]{../res/Lvns_LA_lb.png}
		\caption{Performance profile considering the solution cost.}
		\label{fig:Lsubtour_hardfixing_vns_lb}
	\end{subfigure}
\caption{\texttt{vns} method use \texttt{n\_grasp} with $ n=1 $ as warm start, instead in \texttt{vns\_n\_greedy} and \texttt{vns\_n\_grasp} $ n=10 $. No significant differences to note in terms of execution times and solutions cost between the methods.}
\label{fig:Lvns_pp}
\end{figure}

\section{Genetic Algorithm}
The genetic algorithm (GA) is the last meta-heuristic technique proposed. Its behaviour derives from an evolutionary biology metaphor. A population of individuals (solutions) is randomly created. The individual solutions represent one tour each. These solution are then exposed to simulated evolution.
For a complete explanation of the simple genetic algorithm, see \cite{phdthesis}.\\
A particular genetic algorithm has been implemented in this paper: it is based almost exclusively on the "Powerful Genetic Algorithm Using Edge Assembly Crossover" created by Yuichi Nagata and Shigenobu Kobayashi \cite{Nagata2013, Honda2013}, except for a subsection that has been completely designed by us.\\ A description of the algorithm will be presented and some paragraphs, which are contained in their paper \cite{Nagata2013}, will be reported here thanks to their clear description. In addition, the differences from his algorithm and our implementation choices will be explained.\\
The search process of the GA consists of two stages: \\
\begin{itemize}
\item GA-EAX/Stage I: a localized version of Edge Assembly Crossover (EAX) as the crossover operator from the start of the search until no improvement in the best solution is found over a period of generations or because of timelimit. % l'algoritmo termina davvero se non trova miglioramenti?
\item  GA-EAX/Stage II: after that, switch to a global version of EAX and use it until the end of the search. Stage II is also terminated by the same condition of previous one.
\end{itemize}
For this project only Stage I was developed but the global version does not add any difficulties in the code.

Algorithm \ref{alg:gagen} describes in a compact way the various steps of the entire search.\\

\begin{algorithm}
\caption{GA General}\label{alg:gagen}
\begin{algorithmic}[1]
\Procedure{Procedure GA()}{}
\State $\textit{\{x\textsubscript{$1$},...,x\textsubscript{N\textsubscript{pop}}\}} := \textit{INIT\_POPULATION()}$
\While{\textit{termination condition is satisfied}}
	\State $\textit{r($\cdot$)} := \textit{SHUFFLE\_INDIVIDUALS() $\equiv$ a random permutation of $1$,...,N\textsubscript{pop} } $
	\For{\texttt{$i := 1$ to N\textsubscript{pop}}}
		\State $p_A := \textit{x\textsubscript{r($i$)}} , \textit{p\textsubscript{B}} := \textit{x\textsubscript{r($i+1$)}} $
		\State $\textit{\{y\textsubscript{$1$},...,y\textsubscript{N\textsubscript{kids}}\}} := \textit{EAX\_SINGLE(p\textsubscript{A}, p\textsubscript{B})}$
		\State $x_{r(i)} := SURVIVAL\_SELECTION(y_1,...,y_{N_{kids}}, p_A) $
	\EndFor
	\State $\textit{best\_individual} := \textit{best individual of actual population}$
\EndWhile
\State \textbf{return} $best\_individual$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{EAX\_SINGLE}
The recombination operator EAX uses the edges from the two parents to construct disjoint subtours.
Then, using a more general version of the Patching Algorithm \ref{section:patching}, the subtours are connected in a greedy fashion to produce the offspring tour. Thus, the EAX operator considers local information which is exploited in determining which edges to use to connect subtours.\\
Another important trait of the EAX operator is that it will introduce new edges into the offspring when connecting subtours. Edges not in the parents, or perhaps not even in the population, are introduced into offspring. 
The argument as to why good new edges must be introduced during recombination is simple. As point out in \cite{Mathias92geneticoperators}, the complete graph of all possible edges for a sTSP has $(N^2-N)/2$ edges, where $N$ is the number of nodes. Each tour samples $N$ of these edges, so a population must be of size at least $(N-1)/2$ in order to sample each edge exactly once. % perchè è necessario avere dei tour che passino almeno una volta su ogni arco?
Assume population size is proportional to the number of the nodes. Then each edge occurs twice in expectation in an initial random population. Selection can therefore quickly eliminate edges from the population. Good edges can also be lost if they occur in poor tours. Thus it is important for operators to intelligently introduce new good edges. This feature is part of the construction of EAX and therefore, may contribute to its effectiveness.

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\columnwidth]{img/GA_steps}
	\caption{Example of the EAX\_SINGLE method steps (image taken from \cite{Nagata2013}).}
	\label{fig:GA_steps}
\end{figure}

Focusing on details of the EAX procedure:\\
Once two parents have been selected for crossover, the EAX operator merges these two individuals into a single graph denoted by R (Step 1 of Image \ref{fig:GA_steps}, graph R is named G\textsubscript{AB}). The two parents are denoted by A and B, respectively. Each edge in R is annotated with the parent to which it belongs. R may contain two instances of the same edge, if both parents contain the edge (that is why a more general version of Patching Algorithm is used).\\
R is next divided into a set of disjoint subtours. Let v\textsubscript{i} represent a vertex from R and let (v\textsubscript{i},  v\textsubscript{j}), i $\neq$ j, represent an edge. Suppose (v\textsubscript{i},  v\textsubscript{j}) represents an edge randomly chosen from parent A.\\
Choose one vertex (either v\textsubscript{i} or v\textsubscript{j}) as the origin. If v\textsubscript{i} is the origin, then choose an edge which leads from the second vertex, v\textsubscript{j} , to any other vertex in R. However, this edge must come from parent B. If more than one such edge exists, a random selection is made. The algorithm continues to traverse R, at each step alternately picking edges from parent A and parent B.\\
After each edge is traversed, the algorithm checks to see if adding this new edge to the set of previously selected edges will result in an AB-cycle. An AB-cycle is a even-length sub-cycle of R with edges that alternately come from A and B. An AB-cycle may repeat nodes, but not edges. While there can be two edges between a pair of nodes, they are uniquely identified as an A or B edge, and thus distinct.\\
Once an AB-cycle has been found it is stored and the edges making up that cycle are removed from R. The algorithm repeats this procedure until R contains no more edges, having been completely decomposed into a set of AB-cycles (Step 2 of Image \ref{fig:GA_steps}). \\
The first several edges used in the construction of the AB-cycle may not appear in the final AB-cycle.
This occurs when the final edge connects back onto the subgraph at some node x other than the origin node, and the induced subcycle is an AB-cycle. In this case the remaining edges are however removed from R graph but is kept aside in the path being traced, to eventually be used later in forming another cycle. Nagata and Kobayashi choose an edge incident with x from R to begin construction of the next AB-cycle instead in our algorithm we preferred to take an edge incident to any node remaining in the traced path (other technique prefers to select the starting location of a new AB-cycle at random from R).
For the symmetric TSP problem, R is undirected and therefore, the set of AB-cycles is not uniquely determined by the algorithm. Furthermore, a number of "ineffective" AB-cycles may be formed by the algorithm. All AB-cycles that have less than 4 different nodes inside them belong to this group. Any ineffective AB-cycles are found and removed from R and also removed from consideration by the remaining phases of the algorithm.
\\
After construction of the set of AB-cycles, a subset of AB-cycles is chosen to be used in the generation of an intermediate child. This subset is called an E-set (Step 3 of Image \ref{fig:GA_steps}). For selecting AB-cycles for inclusion into the E-set, we choose an our method different from those defined by Nagata and Kobayashi. Given the small size of the problems we tested compared to those of the TSP Art of Nagata, the number of subtours in the E-set is not very large and therefore, instead of sampling by random selection some of them, we scroll through them all and create one intermediate child for each.
\\
Construction of an intermediate child, C, begins with a copy of parent A. Then each edge of each subtour in the E-set is examined, with the following actions taken on C. If the edge from the E-set is a member of parent A, the edge is deleted from C. If the edge is a member of parent B, the edge is added to C. The result is a set of disjoint subtours which comprise the intermediate child (Step 4 of Image \ref{fig:GA_steps}). \\
The last stage of the EAX operator involves transformation of the intermediate child into a single legal tour using a general Patching Algorithm (Step 5 of Image \ref{fig:GA_steps}). The difference between this general algorithm and the one presented in \ref{section:patching} is in the possibility of having double edges, after performing Step 4. This implies that one of the subtours can only be made up of two nodes and two same edges, one of parent A and one of B. Similar situation with three nodes and six edges (three doubles). In previous problems this situation never happened and therefore we generalized the Patching algorithm to include this variant.\\ 
The pseudocode of the EAX\_SINGLE method is identical to that explained in Algorithm 1 of Paper \cite{Honda2013}.

\subsection{Evaluate AB-Cycles}
This method was completely designed by us from scratch. It refers to how to find a cycle that respects the conditions of the EAX\_SINGLE procedure inside the nodes and edges traced so far by the current AB-Cycle. An explanation of this procedure is not given in the papers \cite{Nagata2013, Honda2013} and it is not even that simple. We initially thought of using a greedy graph search algorithm, such as Depth First Traversal (DFT) or Breadth First Search (BFS), but these algorithms do not include the presence of double edges between two nodes and are based on visiting the nodes and not edges and also once found any cycle they stop and do not provide the possibility to continue the search if the cycle does not meet certain conditions. So we have developed an ad hoc algorithm for this type of search that transforms the graph into a tree considering double edges and the possibility to continue the search if a cycle is found that does not respect the conditions required by the EAX\_SINGLE. This algorithm has a recursive structure and uses linked lists that allow a dynamic allocation to save all the necessary information, it was also built trying to free up memory as much as possible in order to avoid overflow as the generations increase. Furthermore, the possibility of finding all the cycles within the graph and not only one is managed, but, as required by the EAX, when an acceptable solution is found, the search ends.
% spiegare un attimo come funziona? quindi le diramazioni rappresentano ognuna un percorso possibile, in questo modo vengono considerati tutti i possibili cammini all'interno del grafo.

\subsection{SURVIVAL\_SELECTION}
For the survival selection method, some parameters are defined: 

\begin{itemize}
\item N\textsubscript{pop}
\item N\textsubscript{kids}
\item Edge Frequency Table F($e$)
\end{itemize}
N\textsubscript{pop} and N\textsubscript{kids} be the population size and the number of offspring solutions generated from a single pair of parents, p\textsubscript{A} and p\textsubscript{B}, respectively, with the chosen values of 300 and 30, as in the default configuration of GA-EAX/Stage I \cite{Nagata2013}. N\textsubscript{kids} represents an upper bound to the number of offspring solutions but fewer of them could be generated. \\
The edge frequency table F($e$) is a table that records the frequencies of each edge $e \in E$ included in the population, where $E$ is the edge set of the complete graph of a given TSP instance. The values of F($e$) are initialized and are used in the evaluation function for selecting offspring solutions. This evaluation function is based on the edge entropy measure computed from F($e$) and is used for maintaining the population diversity in a positive manner. \\
To keep the table updated: let $y\ssymbol{1}$ be the selected individual among the generated offsprings, which replaces the population member chosen as parent p\textsubscript{A}. The values of F($e$) are updated as follows: 

\begin{equation}\begin{array}{ll}
F(e) \leftarrow F(e)-1 & \forall e \in E\textsubscript{remove} \\
F(e) \leftarrow F(e)+1 & \forall e \in E\textsubscript{add}
\end{array}\end{equation}

where E\textsubscript{remove} is a set of the edges that are included in p\textsubscript{A} but not included in $y\ssymbol{1}$, E\textsubscript{add} is a set of the edges that are included in $y\ssymbol{1}$ but not included in p\textsubscript{A}. \\

The offspring $y\ssymbol{1}$ is selected, taking account of the balance between the amount of the improvement and loss of the population diversity. Let L be the average tour length of the population and H the edge entropy of the population defined as follows:

\begin{equation}
H=-\sum_{e \in E} F(e) / N_{\mathrm{pop}}\left(\log \left(F(e) / N_{\mathrm{pop}}\right)\right)
\end{equation}

$\Delta$L(y) and $\Delta$H(y) denote the differences in L and H, respectively, when x\textsubscript{i}(p\textsubscript{A}) is replaced with $y\ssymbol{1}$. The offspring $y\ssymbol{1}$ is selected so that the following evaluation function is maximized.

\begin{equation}\text { Eval\textsubscript{Ent}}(y):=\left\{\begin{array}{ll}
\frac{\Delta L(y)}{\Delta H(y)} & (\Delta L<0, \Delta H<0) \\
-\frac{\Delta L(y)}{\epsilon} & (\Delta L<0, \Delta H \geq 0) \\
-\Delta L(y), & (\Delta L \geq 0)
\end{array}\right.\end{equation}

where $y$ is an offspring solution and $\epsilon$ is a sufficiently small positive number (our chosen value $0.1$).\\

\subsection{Test}

